{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_xhvUO10XD6"
      },
      "source": [
        "# Hugging Face の利用方法"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82Sx06lGrg3K"
      },
      "source": [
        "## Hugging Face とは"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-S6ghDZpmSG"
      },
      "source": [
        "Hugging Face とは主にトランスフォーマーに関連する深層学習モデルやそれに付随する技術の提供を行っているサービスです．トランスフォーマー（のエンコーダーとデコーダー）はニューラルネットワーク等からなる，様々なタイプのデータを処理できる機械学習モデルです．トランスフォーマーは色々な問題を解決するために少しずつ違う構造を持ちますが，そのようなものをまとめてコマンド一発で利用可能にしてくれているのが Hugging Face です．また，自然言語処理の分野で扱うデータのサイズはとても大きい場合があるのですが，どこかの誰かがあらかじめ何かのデータを利用して学習済みのデータを提供してくれていることがあり，そのような学習済みモデルも簡単に利用できるように整備してくれています．Hugging Face を利用すれば，特に自然言語処理に関する問題を簡単に解けるようになるため紹介します．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "isuh1_iiygT0"
      },
      "source": [
        "### できること"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtIMcWPodXRj"
      },
      "source": [
        "Hugging Face で扱うことができるタスクは以下に示すものがあります．これ以外にもありますが自然言語処理に関する代表的なタスクを抽出しました．括弧内の文字列は実際に Hugging Face を利用する際に指定するオプションです（後で利用します．）．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRGjF8pHdtO1"
      },
      "source": [
        "    \n",
        "\n",
        "*   感情分析（`sentiment-analysis`）：入力した文章が有する感情を予測\n",
        "*   特徴抽出（`feature-extraction`）：入力した文章をその特徴を示すベクトルに変換\n",
        "*   穴埋め（`fill-mask`）：文章中のマスクされた単語を予測\n",
        "*   固有表現抽出（`ner`）：入力した文章中の固有表現（名前とか場所とか）にラベルをつける\n",
        "*   質問応答（`question-answering`）：質問文とその答えが含まれる何らかの説明文を入力として解答文を生成\n",
        "*   要約（`summarization`）：入力した文章を要約\n",
        "*   文章生成（`text-generation`）：文章を入力にして，その文章に続く文章を生成\n",
        "*   翻訳（`translation`）：文章を他の言語に翻訳\n",
        "*   ゼロショット文章分類（`zero-shot-classification`）：文章とそれが属する可能性があるいくつかのカテゴリを入力にしてその文章をひとつのカテゴリに分類\n",
        "    \n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3mzuNEjy5to"
      },
      "source": [
        "### Hugging Face Course"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7fpi958zLiS"
      },
      "source": [
        "Hugging Face の利用方法は以下のウェブサイト，Hugging Face Course の Transformer models の部分を読めば大体のことが把握できると思います．\n",
        "\n",
        "https://huggingface.co/course/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hezey89MynbX"
      },
      "source": [
        "### インストール"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IG4DcIBO42U_"
      },
      "source": [
        "Hugging Face（のトランスフォーマー）は TensorFlow または PyTorch とあわせて利用可能なライブラリです．本来は TensorFlow か PyTorch をあらかじめインストールする必要があります．グーグルコラボラトリーには既に TensorFlow がインストールされているため不要です．以下のように `transformers` のみをインストールすれば利用可能です．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qp0yFo3Se8PB"
      },
      "outputs": [],
      "source": [
        "! pip3 install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYrJO84QpmSM"
      },
      "source": [
        "```{attention}\n",
        "このコンテンツの実行にグーグルコラボラトリーを使っておらず，自身の環境で Anaconda 等を利用している場合は気をつけてください．例えば，Anaconda を利用しているのであれば ` conda install -c huggingface transformers ` のようなコマンドでインストールした方が良いです．\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5RlLdfs5ynbW"
      },
      "source": [
        "## 基本的な使い方"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZpHl3AlpmSN"
      },
      "source": [
        "とても簡単に自然言語処理を実現することができる利用方法を紹介します．世界最高性能を求めたいとかでないなら，ここで紹介する方法を利用して様々なことを達成できると思います．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "saOu-Onk5d-w"
      },
      "source": [
        "### 感情分析"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_RY-XjT5d-w"
      },
      "source": [
        "最も簡単な `tranformers` の利用方法は以下のようになると思います．`pipeline` を読み込んで，そこに取り組みたいタスク（`sentiment-analysis`）を指定します．初回の起動の際には事前学習済みモデルがダウンロードされるため時間がかかります．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K4QgsbGyfBA3"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "from transformers import pipeline\n",
        " \n",
        "def main():\n",
        "    classifier = pipeline(\"sentiment-analysis\")\n",
        "    text = \"I have a pen.\"\n",
        "    result = classifier(text)\n",
        "    print(result)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DE4KbHIJHDFk"
      },
      "source": [
        "入力した文章がポジティブな文章なのかネガティブな文章なのかを分類できます．ここでは1個の文章を入力しましたが，以下のように2個以上の文章も入力可能です．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pVrVwjvFHVax"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "from transformers import pipeline\n",
        " \n",
        "def main():\n",
        "    classifier = pipeline(\"sentiment-analysis\")\n",
        "    litext = [\"I've been waiting for a HuggingFace course my whole life.\", \"I hate this so much!\"]\n",
        "    result = classifier(litext)\n",
        "    print(result)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEBdCL6KcIo8"
      },
      "source": [
        "### 特徴抽出"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5N2rUuWcIo_"
      },
      "source": [
        "文字列の特徴を抽出して何らかの分散表現にしたい場合は `feature-extraction` を指定します．文字列の長さに依存した配列が出力されますが，同じ長さのベクトルにしたい場合は配列長に渡って要素の値を足し算すること等で特徴量を得ることができます．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TbEy3hgncIpA"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "from transformers import pipeline\n",
        " \n",
        "def main():\n",
        "    converter = pipeline(\"feature-extraction\")\n",
        "    text = \"We are very happy to introduce pipeline to the transformers repository.\"\n",
        "    result = converter(text)\n",
        "    print(result)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fIhMpetYd9SW"
      },
      "source": [
        "```{hint}\n",
        "例えば，BERT を使うと各トークンが768次元のベクトルでトークン数個からなる要素のベクトルが出力されます．\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjztlAuqHzky"
      },
      "source": [
        "### ゼロショット文章分類"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7M6_h785Hzky"
      },
      "source": [
        "ゼロショット文章分類は以下のように利用します．ゼロショットとは訓練中に一度も出現しなかったクラスの分類タスクです．ラベルが未定義の問題に利用することができます．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mAXUlTWnIEVE"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "from transformers import pipeline\n",
        " \n",
        "def main():\n",
        "    classifier = pipeline(\"zero-shot-classification\")\n",
        "    text = \"This is a course about the Transformers library\",\n",
        "    lilabel = [\"education\", \"politics\", \"business\"]\n",
        "    result = classifier(text, candidate_labels = lilabel)\n",
        "    print(result)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfsZyVRAIbZo"
      },
      "source": [
        "```{note}\n",
        "ゼロショット分類では，例えば，馬を入力にして猫とか犬とかのラベルそのものを予測されるのではなくて猫および犬ベクトルを予測させどちらに近いかを予測させることができます．そろそろマシンパワー的にきついかもしれませんね．\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eW9CmBX5p2K"
      },
      "source": [
        "### 文章生成"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FgHhNYYjiTvR"
      },
      "source": [
        "文章の生成は以下のように行います．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ean4ulzOIy54"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "from transformers import pipeline\n",
        " \n",
        "def main():\n",
        "    generator = pipeline(\"text-generation\")\n",
        "    text = \"In this course, we will teach you how to\"\n",
        "    result = generator(text)\n",
        "    print(result)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13EIb-OP5xoz"
      },
      "source": [
        "### 穴埋め"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWPChdEo5l_x"
      },
      "source": [
        "穴埋めは以下のように行います．以下のコードでは可能性が高いものふたつを表示させます．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_4MA5Uw6JYiw"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "from transformers import pipeline\n",
        " \n",
        "def main():\n",
        "    unmasker = pipeline(\"fill-mask\")\n",
        "    text = \"This course will teach you all about <mask> models.\"\n",
        "    result = unmasker(text, top_k=2)\n",
        "    print(result)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5morCbD95ytz"
      },
      "source": [
        "### 固有表現抽出"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDj3vKYP5yt0"
      },
      "source": [
        "固有表現抽出は以下のように行います．結果の `PER` は人名，`ORG` は組織名，`LOC` は地名です．それらの固有表現の文書中における位置も `start` と `end` で示されています．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vKnlUcasJeVW"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "from transformers import pipeline\n",
        " \n",
        "def main():\n",
        "    ner = pipeline(\"ner\", grouped_entities=True)\n",
        "    text = \"My name is Sylvain and I work at Hugging Face in Brooklyn.\"\n",
        "    result = ner(text)\n",
        "    print(result)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TtflLBrq5zhl"
      },
      "source": [
        "### 質問応答"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRqwDbBK5zhl"
      },
      "source": [
        "SQuAD のような機械学習コンテストで行われる質問応答は質問だけを入力にして何かを出力する問題ではありません．質問文と何らかの説明文を入力にして解答を出力される問題です．以下のように利用します．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rJdXGoVRJfG_"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "from transformers import pipeline\n",
        " \n",
        "def main():\n",
        "    question_answerer = pipeline(\"question-answering\")\n",
        "    question_text = \"Where do I work?\"\n",
        "    explanation = \"My name is Sylvain and I work at Hugging Face in Brooklyn\"\n",
        "    result = question_answerer(question = question_text, context = explanation)\n",
        "    print(result)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-GL03sOJj2z"
      },
      "source": [
        "### 要約"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GgSz_UKJj2z"
      },
      "source": [
        "文章の要約は以下のように行います．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_BarS-2rN0u7"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "from transformers import pipeline\n",
        " \n",
        "def main():\n",
        "    summarizer = pipeline(\"summarization\")\n",
        "    text = \"\"\"\n",
        "        America has changed dramatically during recent years. Not only has the number of \n",
        "    graduates in traditional engineering disciplines such as mechanical, civil, \n",
        "    electrical, chemical, and aeronautical engineering declined, but in most of \n",
        "    the premier American universities engineering curricula now concentrate on \n",
        "    and encourage largely the study of engineering science. As a result, there \n",
        "    are declining offerings in engineering subjects dealing with infrastructure, \n",
        "    the environment, and related issues, and greater concentration on high \n",
        "    technology subjects, largely supporting increasingly complex scientific \n",
        "    developments. While the latter is important, it should not be at the expense \n",
        "    of more traditional engineering.\n",
        "\n",
        "    Rapidly developing economies such as China and India, as well as other \n",
        "    industrial countries in Europe and Asia, continue to encourage and advance \n",
        "    the teaching of engineering. Both China and India, respectively, graduate \n",
        "    six and eight times as many traditional engineers as does the United States. \n",
        "    Other industrial countries at minimum maintain their output, while America \n",
        "    suffers an increasingly serious decline in the number of engineering graduates \n",
        "    and a lack of well-educated engineers.\n",
        "    \"\"\"\n",
        "    result = summarizer(text)\n",
        "    print(result)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FU_diMPDJ8Je"
      },
      "source": [
        "### 翻訳"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRwU4giyJ8Jf"
      },
      "source": [
        "翻訳はこれまでと少しだけ指定方法が異なります．以下のように `translation_XX_to_YY` としなければなりません．ここでは，英語からフランス語への翻訳を行います．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Iv7oI1PJ8Jf"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "from transformers import pipeline\n",
        " \n",
        "def main():\n",
        "    translator = pipeline(\"translation_en_to_fr\")\n",
        "    text = \"This course is produced by Hugging Face.\"\n",
        "    result = translator(text)\n",
        "    print(result)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqwWY1c0O02R"
      },
      "source": [
        "## 応用的な使い方"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pjsr5ACupmST"
      },
      "source": [
        "これまでに利用したものとは異なるモデルを利用したいとか，自身が持っているデータセットにより適合させたいとかの応用的な利用方法を紹介します．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSKzyoA1O02W"
      },
      "source": [
        "### 他のモデルの利用"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G316vauoido7"
      },
      "source": [
        "これまでに，Hugging Face が自動でダウンロードしてくれたデフォルトの事前学習済みモデルを利用した予測を行いましたが，そうでなくて，モデルを指定することもできます．以下のページをご覧ください．Model Hub と言います．\n",
        "\n",
        "https://huggingface.co/models?pipeline_tag=text-generation&sort=downloads\n",
        "\n",
        "\n",
        "この Model Hub の Tasks というところでタグを選択できます．例えば，Text Generation の `distilgpt2` を利用するには以下のように書きます．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aEduyYGdPhf0"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "from transformers import pipeline\n",
        " \n",
        "def main():\n",
        "    generator = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
        "    text = \"In this course, we will teach you how to\"\n",
        "    result = generator(text)\n",
        "    print(result)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Z4xVexiVKfN"
      },
      "source": [
        "### 日本語の解析"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wK2cF2YVKfR"
      },
      "source": [
        "日本語も扱うことができます．ここでは，日本語で書かれた文章の感情分析を行います．最初に，必要なライブラリをダウンロードしてインストールします．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qknFQF2XVe9e"
      },
      "outputs": [],
      "source": [
        "! pip install fugashi\n",
        "! pip install ipadic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-odvcOynXvTH"
      },
      "source": [
        "```{note}\n",
        "これをインストールしないで使ったらインストールしろってメッセージが出たからインストールしました．\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4Zjx7FQpmSU"
      },
      "source": [
        "```{attention}\n",
        "グーグルコラボラトリーでのインストール方法です．\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLSh2N3ZX6r7"
      },
      "source": [
        "Model Hub で調べたら以下のような感情分析のモデルが公開されていたので，それを使います．トークナイザーとは文章をトークン化（単語化して数字を割り当てます）してくれるものです．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MGPJoEbVVKfR"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "from transformers import pipeline\n",
        "\n",
        "def main():\n",
        "    classifier = pipeline(\"sentiment-analysis\", model=\"daigo/bert-base-japanese-sentiment\", tokenizer=\"daigo/bert-base-japanese-sentiment\")\n",
        "    text = \"みんながマリオのチョコエッグツイートをしていく中，未だにひとつも買えなくて焦る…．\"\n",
        "    result = classifier(text)\n",
        "    print(result)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A99SjtkzYGhL"
      },
      "source": [
        "以下のようにモデルやトークナイザーは明示的に書くこともできます．ここでは，東北大学の乾研が公開している日本語版BERTを利用してみました．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8AicM55hXPPS"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "from transformers import pipeline, AutoTokenizer\n",
        "\n",
        "def main():\n",
        "    mytokenizer = AutoTokenizer.from_pretrained(\"cl-tohoku/bert-base-japanese-whole-word-masking\")\n",
        "    unmasker = pipeline(\"fill-mask\", model=\"cl-tohoku/bert-base-japanese-whole-word-masking\", tokenizer=mytokenizer)\n",
        "    text = \"みんなが[MASK]のチョコエッグツイートをしていく中，未だにひとつも買えなくて焦る…．\"\n",
        "    result = unmasker(text)\n",
        "    print(result)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_UNdPpS3qMjv"
      },
      "source": [
        "### モデルの設定変更"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yt3UNnsZqMjz"
      },
      "source": [
        "上で紹介したのと同様に，モデルとトークナイザーをそれぞれ，`TFAutoModelForSequenceClassification` と `AutoTokenizer` で読み込むことができます．以下の通りです．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "riYXQjfbqMjz"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "from transformers import pipeline, TFAutoModelForSequenceClassification, AutoTokenizer\n",
        "\n",
        "def main():\n",
        "    mymodel = TFAutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "    mytokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "    classifier = pipeline(\"sentiment-analysis\", model=mymodel, tokenizer=mytokenizer)\n",
        "    text = \"I do not have a pen but I am happy.\"\n",
        "    result = classifier(text)\n",
        "    print(result)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BnWxkMStaGv"
      },
      "source": [
        "この際に `Auto` を使わずにあらかじめ用意されていいるクラスを明示的に指定することもできます．この場合，以下のように書きます．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-oyaw5dTte0a"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "from transformers import pipeline, TFDistilBertForSequenceClassification, DistilBertTokenizer\n",
        "\n",
        "def main():\n",
        "    mymodel = TFDistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "    mytokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "    classifier = pipeline(\"sentiment-analysis\", model=mymodel, tokenizer=mytokenizer)\n",
        "    text = \"I do not have a pen but I am happy.\"\n",
        "    result = classifier(text)\n",
        "    print(result)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_DAQNqrtnxe"
      },
      "source": [
        "さらにモデルをカスタマイズすることもできます．ここでは，`DistilBert` の構造を変えてしまっているので，全く性能が出ていません．学習をし直す必要がありそうです．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "niN8Oniptu20"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "from transformers import pipeline, TFDistilBertForSequenceClassification, DistilBertTokenizer, DistilBertConfig\n",
        "\n",
        "def main():\n",
        "    myconfig = DistilBertConfig(n_heads=8, dim=512, hidden_dim=4*512)\n",
        "    mymodel = TFDistilBertForSequenceClassification(myconfig)\n",
        "    mytokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "    classifier = pipeline(\"sentiment-analysis\", model=mymodel, tokenizer=mytokenizer)\n",
        "    text = \"I do not have a pen but I am happy.\"\n",
        "    result = classifier(text)\n",
        "    print(result)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQwtexEFY0jp"
      },
      "source": [
        "### ファインチューニング"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i__HcsHYY0ju"
      },
      "source": [
        "事前学習モデルを自分の解きたい問題に合わせてファインチューニングすることができます．ここでは，インターネット上の映画レビューのデータセット IMDb に対してモデルのファインチューニングを行います．最初に Hugging Face が提供してくれているデータセットを利用するためのモジュールをインストールします．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c9CAu2TXwo5a"
      },
      "outputs": [],
      "source": [
        "! pip3 install datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spMlnpdgwyzM"
      },
      "source": [
        "中身を確認します．映画に関するレビューが含まれています．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c5DEA0mrwxdl"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "from datasets import load_dataset\n",
        "\n",
        "def main():\n",
        "    imdb = load_dataset(\"imdb\")\n",
        "    print(imdb[\"train\"][0])\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ak6bsiQbxTjd"
      },
      "source": [
        "このデータセットをトークナイズします．以下のようなコードを追加します．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lyITun0txTjd"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "def main():\n",
        "    imdb = load_dataset(\"imdb\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "    def preprocess_function(examples):\n",
        "        return tokenizer(examples[\"text\"], truncation=True)\n",
        "    tokenized_imdb = imdb.map(preprocess_function, batched=True)\n",
        "    print(tokenized_imdb)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMbsATli53L7"
      },
      "source": [
        "データセット中のデータをパディングします．データセット中の最大の長さのデータに合わせてパディングしても良いのですが，それだと非効率的なので，バッチ毎にパディングする方法を行います．ダイナミックパディングと呼ばれる方法です．以下のようにします．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PKnzh27_6P6u"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
        "\n",
        "def main():\n",
        "    imdb = load_dataset(\"imdb\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "    def preprocess_function(examples):\n",
        "        return tokenizer(examples[\"text\"], truncation=True)\n",
        "    tokenized_imdb = imdb.map(preprocess_function, batched=True)\n",
        "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2dQT4Zv8iEC"
      },
      "source": [
        "ファインチューニング前のモデルを呼び出して，テストデータセットの最初の5個について感情分析をしてみます．ポジティブとネガティブの判定はどれも 0.5 くらいであり，判別しきれていません．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Au8VIOje7Ct6"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, DataCollatorWithPadding, AutoModelForSequenceClassification, pipeline\n",
        "\n",
        "def main():\n",
        "    imdb = load_dataset(\"imdb\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "    def preprocess_function(examples):\n",
        "        return tokenizer(examples[\"text\"], truncation=True)\n",
        "    tokenized_imdb = imdb.map(preprocess_function, batched=True)\n",
        "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n",
        "    classifier = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
        "    print(classifier(imdb[\"test\"][0:5][\"text\"]))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQ86BEs2iyKz"
      },
      "source": [
        "データセットを TensorFlow で処理できるように変換します．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9xQId7M0iy1x"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, DataCollatorWithPadding, create_optimizer, TFAutoModelForSequenceClassification, pipeline\n",
        "import tensorflow as tf\n",
        "\n",
        "def main():\n",
        "    imdb = load_dataset(\"imdb\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "    def preprocess_function(examples):\n",
        "        return tokenizer(examples[\"text\"], truncation=True)\n",
        "    tokenized_imdb = imdb.map(preprocess_function, batched=True)\n",
        "    data_collator = DataCollatorWithPadding(tokenizer, return_tensors=\"tf\")\n",
        "    model = TFAutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n",
        "    classifier = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
        "    print(classifier(imdb[\"test\"][0:5][\"text\"]))\n",
        "\n",
        "    tf_train_dataset = tokenized_imdb[\"train\"].to_tf_dataset(\n",
        "        columns=['attention_mask', 'input_ids', 'label'],\n",
        "        shuffle=True,\n",
        "        batch_size=16,\n",
        "        collate_fn=data_collator,\n",
        "    )\n",
        "\n",
        "    tf_validation_dataset = tokenized_imdb[\"train\"].to_tf_dataset(\n",
        "        columns=['attention_mask', 'input_ids', 'label'],\n",
        "        shuffle=False,\n",
        "        batch_size=16,\n",
        "        collate_fn=data_collator,\n",
        "    )\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsYxHbDIjYr9"
      },
      "source": [
        "学習の条件を設定し，学習を行います．ファインチューニング済みモデルを用いてテストデータセットの最初の5個の予測をしていますが，どれも判別の確率が上がっています．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X0l0ds5pEXNd"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, DataCollatorWithPadding, create_optimizer, TFAutoModelForSequenceClassification, pipeline\n",
        "import tensorflow as tf\n",
        "\n",
        "def main():\n",
        "    imdb = load_dataset(\"imdb\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "    def preprocess_function(examples):\n",
        "        return tokenizer(examples[\"text\"], truncation=True)\n",
        "    tokenized_imdb = imdb.map(preprocess_function, batched=True)\n",
        "    data_collator = DataCollatorWithPadding(tokenizer, return_tensors=\"tf\")\n",
        "    model = TFAutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n",
        "    classifier = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
        "    print(classifier(imdb[\"test\"][0:5][\"text\"]))\n",
        "\n",
        "    tf_train_dataset = tokenized_imdb[\"train\"].to_tf_dataset(\n",
        "        columns=['attention_mask', 'input_ids', 'label'],\n",
        "        shuffle=True,\n",
        "        batch_size=16,\n",
        "        collate_fn=data_collator,\n",
        "    )\n",
        "\n",
        "    tf_validation_dataset = tokenized_imdb[\"train\"].to_tf_dataset(\n",
        "        columns=['attention_mask', 'input_ids', 'label'],\n",
        "        shuffle=False,\n",
        "        batch_size=16,\n",
        "        collate_fn=data_collator,\n",
        "    )\n",
        "\n",
        "    batch_size = 16\n",
        "    num_epochs = 5\n",
        "    batches_per_epoch = len(tokenized_imdb[\"train\"]) // batch_size\n",
        "    total_train_steps = int(batches_per_epoch * num_epochs)\n",
        "    optimizer, schedule = create_optimizer(\n",
        "        init_lr=2e-5, \n",
        "        num_warmup_steps=0, \n",
        "        num_train_steps=total_train_steps\n",
        "    )\n",
        "\n",
        "    model.compile(optimizer=optimizer)\n",
        "    model.fit(\n",
        "        tf_train_dataset,\n",
        "        validation_data=tf_validation_dataset,\n",
        "        epochs=num_epochs,\n",
        "    )\n",
        "\n",
        "    print(classifier(imdb[\"test\"][0:5][\"text\"]))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MY0xwvGBkVad"
      },
      "source": [
        "ファインチューニング済みのモデルやトークナイザーは以下のように保存します．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R9oz2MDHkCcq"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, DataCollatorWithPadding, create_optimizer, TFAutoModelForSequenceClassification, pipeline\n",
        "import tensorflow as tf\n",
        "\n",
        "def main():\n",
        "    imdb = load_dataset(\"imdb\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "    def preprocess_function(examples):\n",
        "        return tokenizer(examples[\"text\"], truncation=True)\n",
        "    tokenized_imdb = imdb.map(preprocess_function, batched=True)\n",
        "    data_collator = DataCollatorWithPadding(tokenizer, return_tensors=\"tf\")\n",
        "    model = TFAutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n",
        "    classifier = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
        "    print(classifier(imdb[\"test\"][0:5][\"text\"]))\n",
        "\n",
        "    tf_train_dataset = tokenized_imdb[\"train\"].to_tf_dataset(\n",
        "        columns=['attention_mask', 'input_ids', 'label'],\n",
        "        shuffle=True,\n",
        "        batch_size=16,\n",
        "        collate_fn=data_collator,\n",
        "    )\n",
        "\n",
        "    tf_validation_dataset = tokenized_imdb[\"train\"].to_tf_dataset(\n",
        "        columns=['attention_mask', 'input_ids', 'label'],\n",
        "        shuffle=False,\n",
        "        batch_size=16,\n",
        "        collate_fn=data_collator,\n",
        "    )\n",
        "\n",
        "    batch_size = 16\n",
        "    num_epochs = 5\n",
        "    batches_per_epoch = len(tokenized_imdb[\"train\"]) // batch_size\n",
        "    total_train_steps = int(batches_per_epoch * num_epochs)\n",
        "    optimizer, schedule = create_optimizer(\n",
        "        init_lr=2e-5, \n",
        "        num_warmup_steps=0, \n",
        "        num_train_steps=total_train_steps\n",
        "    )\n",
        "    \n",
        "    model.compile(optimizer=optimizer)\n",
        "    model.fit(\n",
        "        tf_train_dataset,\n",
        "        validation_data=tf_validation_dataset,\n",
        "        epochs=num_epochs,\n",
        "    )\n",
        "\n",
        "    print(classifier(imdb[\"test\"][0:5][\"text\"]))\n",
        "\n",
        "    save_directory = './pretrained'\n",
        "    tokenizer.save_pretrained(save_directory)\n",
        "    model.save_pretrained(save_directory)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rfC8Kj03omW"
      },
      "source": [
        "```{note}\n",
        "終わりです．\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       m() < self.epsilon:\n",
        "            action = np.random.randint(0, len(self.actions)) # イプシロンの確率でランダムに行動する．\n",
        "        else:\n",
        "            action = np.argmax(self.qValues[self.observation]) # 最もQ値が高い行動を選択．\n",
        "        return action\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2f60YLzfC8Y_"
      },
      "source": [
        "最後の以下の部分は Q テーブルを更新するための記述です．行動選択によって新たなマスへの移動が行われる場合は新たに Q テーブルのキーを生成します．その後，現在の Q 値を計算（参照）し，また，環境遷移後の最も高い Q 値を示す行動の Q 値の値を計算します．これらは，上述の Q 値の更新式で利用する値です．これらを利用して Q 値を更新します．\n",
        "\n",
        "```python\n",
        "    # 以下はQテーブルを更新する関数．\n",
        "    def update(self, objectNewPosition, action, reward):\n",
        "        objectNewPosition = str(objectNewPosition)\n",
        "        if objectNewPosition not in self.qValues: # Qテーブルのキーを新たに作る．\n",
        "            self.qValues[objectNewPosition] = np.repeat(0.0, len(self.actions))\n",
        "        q = self.qValues[self.observation][action]  # Q(s,a)の計算．\n",
        "        maxQ = max(self.qValues[objectNewPosition])  # max(Q(s',a'))の計算．\n",
        "        self.qValues[self.observation][action] = q + (self.alpha * (reward + (self.gamma * maxQ) - q)) # Q'(s, a) = Q(s, a) + alpha * (reward + gamma * maxQ(s',a') - Q(s, a))の計算．\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EgX3GSALHpXK"
      },
      "source": [
        "最後に，`main()` に戻って以下の部分の説明をします．エピソードは 50 回分計算します．環境の初期化をした後に，`agent.act()` によってオブジェクトにさせる行動を選択します．その行動を基に `env.step()` で環境を勧めます．引き続いて Q テーブルの更新を行います．もし，オブジェクトがゴールに達している場合はそれ以上のオブジェクトの移動や Q テーブルの更新を停止します．最後に，ゴールに到達するまでに要した環境遷移の回数と各エピソード毎に得られた報酬の平均値を出力します．\n",
        "\n",
        "```python\n",
        "    for episode in range(1, 50+1):\n",
        "        rewards = []\n",
        "        observation = env.reset() # 環境の初期化．\n",
        "        while True:\n",
        "            action = agent.act(observation) # エージェントによってオブジェクトにさせるアクションを選択する．\n",
        "            observation, reward, done = env.step(action) # 環境を進める．\n",
        "            agent.update(observation, action, reward) # Qテーブルの更新．\n",
        "            rewards.append(reward)\n",
        "            if done: break\n",
        "        print(\"Episode: {:3d}, number of steps: {:3d}, mean reward: {:6.3f}\".format(episode, len(rewards), np.mean(rewards)))\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "saOu-Onk5d-w"
      },
      "source": [
        "### 環境の可視化"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DE4KbHIJHDFk"
      },
      "source": [
        "上のプログラムを実行しただけではエージェントによる行動の選択や環境の遷移によってどのようなことが起こっているのかがよくわかりませんでした．以下のプログラムを動かすとどのように環境が遷移したのかを観察することができます．フィールドの様子を可視化している点以外は上のブログラムと同じものです．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mo4S9m7xQS_e"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "import numpy as np\n",
        "np.random.seed(0)\n",
        "\n",
        "# 環境をステップ毎に描画するようにしたもの．\n",
        "\n",
        "def main():\n",
        "    env = Environment()\n",
        "    observation = env.reset()\n",
        "    agent = Agent(alpha=0.1, epsilon=0.3, gamma=0.9, actions=np.arange(4), observation=observation)\n",
        "    \n",
        "    for episode in range(1, 50+1):\n",
        "        rewards = []\n",
        "        observation = env.reset() # 環境の初期化．\n",
        "        env.render()\n",
        "        while True:\n",
        "            action = agent.act(observation) # エージェントによってオブジェクトにさせるアクションを選択する．\n",
        "            observation, reward, done = env.step(action) # 環境を進める．\n",
        "            env.render()\n",
        "            agent.update(observation, action, reward) # Qテーブルの更新．\n",
        "            rewards.append(reward)\n",
        "            if done: break\n",
        "        print(\"Episode: {:3d}, number of steps: {:3d}, mean reward: {:6.3f}\".format(episode, len(rewards), np.mean(rewards)))\n",
        "\n",
        "class Environment:\n",
        "    def __init__(self):\n",
        "        self.actions = {\"up\": 0, \"down\": 1, \"left\": 2, \"right\": 3}\n",
        "        self.field = [[\"X\", \"X\", \"O\", \"G\"],\n",
        "                      [\"O\", \"O\", \"O\", \"O\"],\n",
        "                      [\"X\", \"O\", \"O\", \"O\"],\n",
        "                      [\"O\", \"O\", \"X\", \"O\"],\n",
        "                      [\"O\", \"O\", \"O\", \"O\"]]\n",
        "        self.done = False\n",
        "        self.reward = None\n",
        "        self.iteration = None\n",
        "    \n",
        "    # 以下は環境を初期化する関数．\n",
        "    def reset(self):\n",
        "        self.objectPosition = 4, 0\n",
        "        self.done = False\n",
        "        self.reward = None\n",
        "        self.iteration = 0\n",
        "        return self.objectPosition\n",
        "    \n",
        "    # 以下は環境を進める関数．\n",
        "    def step(self, action):\n",
        "        self.iteration += 1\n",
        "        y, x = self.objectPosition\n",
        "        if self.checkMovable(x, y, action) == False: # オブジェクトの移動が可能かどうかを判定．\n",
        "            return self.objectPosition, -1, False # 移動できないときの報酬は-1．\n",
        "        else:\n",
        "            if action == self.actions[\"up\"]:\n",
        "                y += -1 # フィールドと座標の都合上，上への移動の場合は-1をする．\n",
        "            elif action == self.actions[\"down\"]:\n",
        "                y += 1\n",
        "            elif action == self.actions[\"left\"]:\n",
        "                x += -1\n",
        "            elif action == self.actions[\"right\"]:\n",
        "                x += 1\n",
        "            # 以下のifは報酬の計算とオブジェクトがゴールに到達してゲーム終了となるかどうかの判定のため．\n",
        "            if self.field[y][x] == \"O\":\n",
        "                self.reward = 0\n",
        "            elif self.field[y][x] == \"G\":\n",
        "                self.done = True\n",
        "                self.reward = 100\n",
        "            self.objectPosition = y, x\n",
        "            return self.objectPosition, self.reward, self.done\n",
        "    \n",
        "    # 以下は移動が可能かどうかを判定する関数．\n",
        "    def checkMovable(self, x, y, action):\n",
        "        if action == self.actions[\"up\"]:\n",
        "            y += -1\n",
        "        elif action == self.actions[\"down\"]:\n",
        "            y += 1\n",
        "        elif action == self.actions[\"left\"]:\n",
        "            x += -1\n",
        "        elif action == self.actions[\"right\"]:\n",
        "            x += 1\n",
        "        if y < 0 or y >= len(self.field):\n",
        "            return False\n",
        "        elif x < 0 or x >= len(self.field[0]):\n",
        "            return False\n",
        "        elif self.field[y][x] == \"X\":\n",
        "            return False\n",
        "        else:\n",
        "            return True\n",
        "    \n",
        "    # 以下はフィールドとオブジェクト（8）の様子を可視化する関数．\n",
        "    def render(self):\n",
        "        y, x = self.objectPosition\n",
        "        field = [[\"X\", \"X\", \"O\", \"G\"],\n",
        "                 [\"O\", \"O\", \"O\", \"O\"],\n",
        "                 [\"X\", \"O\", \"O\", \"O\"],\n",
        "                 [\"O\", \"O\", \"X\", \"O\"],\n",
        "                 [\"O\", \"O\", \"O\", \"O\"]]\n",
        "        field[y][x] = \"8\"\n",
        "        print(\"Iteration = {:3d}\".format(self.iteration))\n",
        "        for i in range(5):\n",
        "            for j in range(4):\n",
        "                print(field[i][j], end=\" \")\n",
        "            print()\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self, alpha=0.1, epsilon=0.3, gamma=0.9, actions=None, observation=None):\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.actions = actions\n",
        "        self.observation = str(observation)\n",
        "        self.qValues = {} # Qテーブル\n",
        "        self.qValues[self.observation] = np.repeat(0.0, len(self.actions))\n",
        "    \n",
        "    # 以下の関数は行動を選択する関数．\n",
        "    def act(self, observation):\n",
        "        self.observation = str(observation)\n",
        "        if np.random.uniform() < self.epsilon:\n",
        "            action = np.random.randint(0, len(self.actions)) # イプシロンの確率でランダムに行動する．\n",
        "        else:\n",
        "            action = np.argmax(self.qValues[self.observation]) # 最もQ値が高い行動を選択．\n",
        "        return action\n",
        "    \n",
        "    # 以下はQテーブルを更新する関数．\n",
        "    def update(self, objectNewPosition, action, reward):\n",
        "        objectNewPosition = str(objectNewPosition)\n",
        "        if objectNewPosition not in self.qValues: # Qテーブルのキーを新たに作る．\n",
        "            self.qValues[objectNewPosition] = np.repeat(0.0, len(self.actions))\n",
        "        q = self.qValues[self.observation][action]  # Q(s,a)の計算．\n",
        "        maxQ = max(self.qValues[objectNewPosition])  # max(Q(s',a'))の計算．\n",
        "        self.qValues[self.observation][action] = q + (self.alpha * (reward + (self.gamma * maxQ) - q)) # Q'(s, a) = Q(s, a) + alpha * (reward + gamma * maxQ(s',a') - Q(s, a))の計算．\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2TPevxmTRguD"
      },
      "source": [
        "プログラムを実行した結果，フィールドが表示され，環境遷移のイタレーションの度に `8` で表示されるオブジェクトがスタート位置からゴール位置へと移動している様子が可視化されました．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yi5MUslnSBVk"
      },
      "source": [
        "```{note}\n",
        "エピソードの数値が小さいとき，つまり，Q テーブルの性能が低い場合は `8` はフラフラしていますね．\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSDtwYfbSEq_"
      },
      "source": [
        "このプログラムが上のプログラムと変わっている点は環境のレンダリングをしているところです．プログラムでは以下の部分に `env.render()` という記述が加わっています．\n",
        "\n",
        "```python\n",
        "    for episode in range(1, 50+1):\n",
        "        rewards = []\n",
        "        observation = env.reset() # 環境の初期化．\n",
        "        env.render()\n",
        "        while True:\n",
        "            action = agent.act(observation) # エージェントによってオブジェクトにさせるアクションを選択する．\n",
        "            observation, reward, done = env.step(action) # 環境を進める．\n",
        "            env.render()\n",
        "            agent.update(observation, action, reward) # Qテーブルの更新．\n",
        "            rewards.append(reward)\n",
        "            if done: break\n",
        "        print(\"Episode: {:3d}, number of steps: {:3d}, mean reward: {:6.3f}\".format(episode, len(rewards), np.mean(rewards)))\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RV0amosSf2v"
      },
      "source": [
        "クラス `Environment` の部分に，`env.render()` がしていることの記述があります．フィールドにオブジェクトの位置を代入してそれを表示するだけですね．\n",
        "\n",
        "```\n",
        "    # 以下はフィールドとオブジェクト（8）の様子を可視化する関数．\n",
        "    def render(self):\n",
        "        y, x = self.objectPosition\n",
        "        field = [[\"X\", \"X\", \"O\", \"G\"],\n",
        "                 [\"O\", \"O\", \"O\", \"O\"],\n",
        "                 [\"X\", \"O\", \"O\", \"O\"],\n",
        "                 [\"O\", \"O\", \"X\", \"O\"],\n",
        "                 [\"O\", \"O\", \"O\", \"O\"]]\n",
        "        field[y][x] = \"8\"\n",
        "        print(\"Iteration = {:3d}\".format(self.iteration))\n",
        "        for i in range(5):\n",
        "            for j in range(4):\n",
        "                print(field[i][j], end=\" \")\n",
        "            print()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQQnFSok1Dcc"
      },
      "source": [
        "### Q テーブルの出力"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCma89PIS4Ks"
      },
      "source": [
        "最後に Q テーブルがどんなように成長したのかを以下のプログラムで確認します．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RGRPFR63S8eD"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "import numpy as np\n",
        "np.random.seed(0)\n",
        "\n",
        "# Qテーブルを出力可能にしたもの．\n",
        "\n",
        "def main():\n",
        "    env = Environment()\n",
        "    observation = env.reset()\n",
        "    agent = Agent(alpha=0.1, epsilon=0.3, gamma=0.9, actions=np.arange(4), observation=observation)\n",
        "    \n",
        "    for episode in range(1, 50+1):\n",
        "        rewards = []\n",
        "        observation = env.reset() # 環境の初期化．\n",
        "        env.render()\n",
        "        while True:\n",
        "            action = agent.act(observation) # エージェントによってオブジェクトにさせるアクションを選択する．\n",
        "            observation, reward, done = env.step(action) # 環境を進める．\n",
        "            env.render()\n",
        "            agent.update(observation, action, reward) # Qテーブルの更新．\n",
        "            rewards.append(reward)\n",
        "            if done: break\n",
        "        print(\"Episode: {:3d}, number of steps: {:3d}, mean reward: {:6.3f}\".format(episode, len(rewards), np.mean(rewards)))\n",
        "        agent.outputQTable()\n",
        "\n",
        "class Environment:\n",
        "    def __init__(self):\n",
        "        self.actions = {\"up\": 0, \"down\": 1, \"left\": 2, \"right\": 3}\n",
        "        self.field = [[\"X\", \"X\", \"O\", \"G\"],\n",
        "                      [\"O\", \"O\", \"O\", \"O\"],\n",
        "                      [\"X\", \"O\", \"O\", \"O\"],\n",
        "                      [\"O\", \"O\", \"X\", \"O\"],\n",
        "                      [\"O\", \"O\", \"O\", \"O\"]]\n",
        "        self.done = False\n",
        "        self.reward = None\n",
        "        self.iteration = None\n",
        "    \n",
        "    # 以下は環境を初期化する関数．\n",
        "    def reset(self):\n",
        "        self.objectPosition = 4, 0\n",
        "        self.done = False\n",
        "        self.reward = None\n",
        "        self.iteration = 0\n",
        "        return self.objectPosition\n",
        "    \n",
        "    # 以下は環境を進める関数．\n",
        "    def step(self, action):\n",
        "        self.iteration += 1\n",
        "        y, x = self.objectPosition\n",
        "        if self.checkMovable(x, y, action) == False: # オブジェクトの移動が可能かどうかを判定．\n",
        "            return self.objectPosition, -1, False # 移動できないときの報酬は-1．\n",
        "        else:\n",
        "            if action == self.actions[\"up\"]:\n",
        "                y += -1 # フィールドと座標の都合上，上への移動の場合は-1をする．\n",
        "            elif action == self.actions[\"down\"]:\n",
        "                y += 1\n",
        "            elif action == self.actions[\"left\"]:\n",
        "                x += -1\n",
        "            elif action == self.actions[\"right\"]:\n",
        "                x += 1\n",
        "            # 以下のifは報酬の計算とオブジェクトがゴールに到達してゲーム終了となるかどうかの判定のため．\n",
        "            if self.field[y][x] == \"O\":\n",
        "                self.reward = 0\n",
        "            elif self.field[y][x] == \"G\":\n",
        "                self.done = True\n",
        "                self.reward = 100\n",
        "            self.objectPosition = y, x\n",
        "            return self.objectPosition, self.reward, self.done\n",
        "    \n",
        "    # 以下は移動が可能かどうかを判定する関数．\n",
        "    def checkMovable(self, x, y, action):\n",
        "        if action == self.actions[\"up\"]:\n",
        "            y += -1\n",
        "        elif action == self.actions[\"down\"]:\n",
        "            y += 1\n",
        "        elif action == self.actions[\"left\"]:\n",
        "            x += -1\n",
        "        elif action == self.actions[\"right\"]:\n",
        "            x += 1\n",
        "        if y < 0 or y >= len(self.field):\n",
        "            return False\n",
        "        elif x < 0 or x >= len(self.field[0]):\n",
        "            return False\n",
        "        elif self.field[y][x] == \"X\":\n",
        "            return False\n",
        "        else:\n",
        "            return True\n",
        "    \n",
        "    # 以下はフィールドとオブジェクト（8）の様子を可視化する関数．\n",
        "    def render(self):\n",
        "        y, x = self.objectPosition\n",
        "        field = [[\"X\", \"X\", \"O\", \"G\"],\n",
        "                 [\"O\", \"O\", \"O\", \"O\"],\n",
        "                 [\"X\", \"O\", \"O\", \"O\"],\n",
        "                 [\"O\", \"O\", \"X\", \"O\"],\n",
        "                 [\"O\", \"O\", \"O\", \"O\"]]\n",
        "        field[y][x] = \"8\"\n",
        "        print(\"Iteration = {:3d}\".format(self.iteration))\n",
        "        for i in range(5):\n",
        "            for j in range(4):\n",
        "                print(field[i][j], end=\" \")\n",
        "            print()\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self, alpha=0.1, epsilon=0.3, gamma=0.9, actions=None, observation=None):\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.actions = actions\n",
        "        self.observation = str(observation)\n",
        "        self.qValues = {} # Qテーブル\n",
        "        self.qValues[self.observation] = np.repeat(0.0, len(self.actions))\n",
        "    \n",
        "    # 以下の関数は行動を選択する関数．\n",
        "    def act(self, observation):\n",
        "        self.observation = str(observation)\n",
        "        if np.random.uniform() < self.epsilon:\n",
        "            action = np.random.randint(0, len(self.actions)) # イプシロンの確率でランダムに行動する．\n",
        "        else:\n",
        "            action = np.argmax(self.qValues[self.observation]) # 最もQ値が高い行動を選択．\n",
        "        return action\n",
        "    \n",
        "    # 以下はQテーブルを更新する関数．\n",
        "    def update(self, objectNewPosition, action, reward):\n",
        "        objectNewPosition = str(objectNewPosition)\n",
        "        if objectNewPosition not in self.qValues: # Qテーブルのキーを新たに作る．\n",
        "            self.qValues[objectNewPosition] = np.repeat(0.0, len(self.actions))\n",
        "        q = self.qValues[self.observation][action]  # Q(s,a)の計算．\n",
        "        maxQ = max(self.qValues[objectNewPosition])  # max(Q(s',a'))の計算．\n",
        "        self.qValues[self.observation][action] = q + (self.alpha * (reward + (self.gamma * maxQ) - q)) # Q'(s, a) = Q(s, a) + alpha * (reward + gamma * maxQ(s',a') - Q(s, a))の計算．\n",
        "    \n",
        "    # 以下はQテーブルを出力する関数．\n",
        "    def outputQTable(self):\n",
        "        print(\"Q-table:    Up   Down   Left  Right\")\n",
        "        for key in sorted(self.qValues.keys()):\n",
        "            print(key, end=\" \")\n",
        "            for j in range(4):\n",
        "                print(\"{:7.3f}\".format(self.qValues[key][j]), end=\"\")\n",
        "            print()\n",
        "        print()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOKb7g25TN9p"
      },
      "source": [
        "これは Q テーブルを出力する記述を加えただけなので説明は不要かもしれませんが，Q テーブルを出力するための記述はクラス `Agent` の以下の部分に追加しました．\n",
        "\n",
        "```python\n",
        "    # 以下はQテーブルを出力する関数．\n",
        "    def outputQTable(self):\n",
        "        print(\"Q-table:    Up   Down   Left  Right\")\n",
        "        for key in sorted(self.qValues.keys()):\n",
        "            print(key, end=\" \")\n",
        "            for j in range(4):\n",
        "                print(\"{:7.3f}\".format(self.qValues[key][j]), end=\"\")\n",
        "            print()\n",
        "        print()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUryaOsdTFWy"
      },
      "source": [
        "全てのエピソードが終了した後の Q テーブルは以下のようなものとなりました．例えば，ゴール付近のマスである，`(0, 2)` や `(1, 3)` ではどのような行動が高い Q 値を持つかと確認してみると，オブジェクトが `(0, 2)` のときは「右」に移動させることが，`(1, 3)` のときは「上」に移動させることが最も高い Q 値を示していました．良い Q テーブルへと成長したことが確認できます．\n",
        "\n",
        "```\n",
        "Q-table:    Up   Down   Left  Right\n",
        "(0, 2)  10.674  1.824 -0.190 99.030\n",
        "(0, 3)   0.000  0.000  0.000  0.000\n",
        "(1, 0)  -0.100 -0.100 -0.100  7.221\n",
        "(1, 1)  27.443  0.433  0.238 68.646\n",
        "(1, 2)  84.938  7.338 18.591 12.410\n",
        "(1, 3)  46.856  0.000  6.858  1.610\n",
        "(2, 1)  48.270  0.997  9.978 10.193\n",
        "(2, 2)  44.444  0.453  0.000  0.000\n",
        "(3, 0)   1.532  1.509  2.382 20.779\n",
        "(3, 1)  32.544  0.346  0.864  3.671\n",
        "(4, 0)  12.940 -0.254  1.262  0.341\n",
        "(4, 1)   0.457 -0.171  3.350  0.000\n",
        "(4, 2)  -0.271 -0.100  0.041  0.000\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqwWY1c0O02R"
      },
      "source": [
        "## OpenAI Gym"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vjEUHVaiDWW"
      },
      "source": [
        "OpenAI Gym を利用すると様々なゲームを Python からコントロールできるようになります．スーパーマリオブラザーズ等の有名なゲームも Python コード上で構築したエージェントが動かすことができるようになります．たくさんの Atari のゲームも動かすことができます．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJ1PoIfpKRvP"
      },
      "source": [
        "```{hint}\n",
        "ただし，現在の OpenAI Gym には ROM ファイルは同梱されていません．著作権の問題なのかもしれませんが，詳しい事情は知りません．よって，マリオのゲームや Atari のゲームを利用したい場合はどこかからか ROM ファイルを入手する必要があります．\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSKzyoA1O02W"
      },
      "source": [
        "### インストール"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7mwqJn5KyJ7"
      },
      "source": [
        "以下のようなコマンドを打つことで OpenAI Gym をインストールすることができます．グーグルコラボラトリーにはあらかじめインストールされているためこのコマンドは打つ必要はありません．打っても良いです．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bCyMxhmmK_eC"
      },
      "outputs": [],
      "source": [
        "! pip install gym"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6R2Wn48-4NwS"
      },
      "source": [
        "### 環境の生成"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7GaAaw3LHHa"
      },
      "source": [
        "どのような環境（ゲーム）が利用可能であるかは以下のようなコードによって確認できます．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bIr-BAUwLMzS"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "from gym import envs\n",
        "\n",
        "def main():\n",
        "    envids = [spec.id for spec in envs.registry.all()]\n",
        "    print(\"The number of environments:\", len(envs.registry.all())) # 全ての環境の個数を出力．\n",
        "    for envid in envids:\n",
        "        print(envid)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NIFMBsqXNjY"
      },
      "source": [
        "この中の `CartPole-v1` というものを呼び出します．また，環境を初期化し，ランダムな入力から行動選択し，その行動によって環境を遷移させます．以下のように書きます．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D19VYaZQLZ3o"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "import gym\n",
        "\n",
        "def main():\n",
        "    env = gym.make(\"CartPole-v1\")\n",
        "    observation = env.reset() # 環境の初期化，リセット．\n",
        "    print(1,observation)\n",
        "    action = env.action_space.sample() # ランダムな行動選択．強化学習をする際はここはエージェントによる行動選択となる．\n",
        "    observation, reward, done, info = env.step(action) # 環境の遷移．現在の状況に対して行動を行って，次の状態，報酬，停止条件の真偽，ゲーム状態を出力．\n",
        "    print(2,observation)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DnQ7XEdNs19"
      },
      "source": [
        "観測値として出力されている値は 4 個の要素からなるリストですが，最初から「台車の位置」，「台車の速さ」，「棒の角度」，「棒の角速度」です．何のことを言っているのかわからないと思いますが，次の項で可視化すると意味がわかると思います．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TuGl7PZvKWdE"
      },
      "source": [
        "```{note}\n",
        "インターネット上の記事で速度と速さの使い分けがされていなさすぎて驚きました．\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XuTf_DPdOKSY"
      },
      "source": [
        "```{note}\n",
        "この gym を利用した場合に，`reset()` とか `step()` のような書き方があります．上の節で紹介した Q 学習の部分で環境のクラス `Enviroment` にも同様の方法がありましたが，あれは gym の挙動に似せて作ったものです．\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jEn6wq-nasg"
      },
      "source": [
        "状態空間，行動空間，報酬空間は以下のようにすることで確認できます．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zxt2ACminbP7"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "import gym\n",
        "\n",
        "def main():\n",
        "    env = gym.make(\"CartPole-v1\")\n",
        "    env.seed(0)\n",
        "    print(env.observation_space)\n",
        "    print(env.action_space)\n",
        "    print(env.action_space.n)\n",
        "    print(env.reward_range)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7juvlqB9oXhD"
      },
      "source": [
        "以下のようにすると複数の環境を一度に実行することができます．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "feDGoNpHoXpE"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "import gym\n",
        "\n",
        "def main():\n",
        "    env = gym.vector.make(\"CartPole-v1\", 3, asynchronous=True)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QILs9jv_QXla"
      },
      "source": [
        "### 環境遷移の再生"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8CcgtojASgTg"
      },
      "source": [
        "グーグルコラボラトリー可視化を行うためには特殊なコマンドを打って準備をする必要があります．以下のコマンドを打ちます．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o_K7RX9qSpqT"
      },
      "outputs": [],
      "source": [
        "! apt update\n",
        "! apt install xvfb\n",
        "! pip install pyvirtualdisplay"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zO5xH-wbHIy"
      },
      "source": [
        "また，2022 年 10 月 5 日に気づいたところ，以下のライブラリをインストールしないといけなくなったようなので，インストールします．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yo78s-xHas8C"
      },
      "outputs": [],
      "source": [
        "! pip install pygame"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQxfznd6TV2d"
      },
      "source": [
        "以下のプログラムを実行して描画に必要なディスプレイの起動をします．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WwHEF48XTTaa"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "from pyvirtualdisplay import Display\n",
        "\n",
        "def main():\n",
        "    gymDisplay = Display()\n",
        "    gymDisplay.start()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhnjtxW8Ssdi"
      },
      "source": [
        "```{hint}\n",
        "描画をする前にこのコマンドを打たなければなりません．これを打つ前に以下にあるプログラムを実行するとエラーが出ます．気をつけてください．やってしまった場合はグーグルコラボラトリーのランタイムを削除して再起動してやり直してください．\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUmSUqPSTeHu"
      },
      "source": [
        "以下のようなプログラムでゲームの実行画面を描画することができます．ここでは描画の方法を 2 個紹介しますが，その 1 個目です．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WFqjfxhlTyKI"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "import gym\n",
        "from IPython import display\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import animation\n",
        "\n",
        "def main():\n",
        "    env = gym.make('CartPole-v1')\n",
        "\n",
        "    images = [] # 描画のための記述\n",
        "    ax = plt.gca() # 描画をきれいにするための記述\n",
        "    ax.axes.xaxis.set_visible(False) # 描画をきれいにするための記述\n",
        "    ax.axes.yaxis.set_visible(False) # 描画をきれいにするための記述\n",
        "    observation = env.reset()\n",
        "    for _ in range(100):\n",
        "        action = env.action_space.sample()\n",
        "        observation, reward, done, info = env.step(action)\n",
        "\n",
        "        image = plt.imshow(env.render(mode=\"rgb_array\")) # 描画のための記述\n",
        "        images.append([image]) # 描画のための記述\n",
        "\n",
        "        if done: env.reset()\n",
        "\n",
        "    generatedAnimation = animation.ArtistAnimation(plt.gcf(), images, interval=15, blit=True) # 描画のための記述\n",
        "    display.display(display.HTML(generatedAnimation.to_jshtml())) # 描画のための記述\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRXKRVXPYbVx"
      },
      "source": [
        "表示された動画を確認するとわかるように，このゲームは車の上に棒が設置されているもので，その棒が倒れないように車をうまく動かすというものです．次に，もう 1 個の描画方法を紹介します．以下のように書きます．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "LFtmBjMHYmKM"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "import gym\n",
        "from IPython import display\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import animation\n",
        "\n",
        "def main():\n",
        "    env = gym.make(\"MountainCar-v0\")\n",
        "\n",
        "    images = [] # 描画のための記述\n",
        "    observation = env.reset()\n",
        "    for _ in range(100):\n",
        "        action = env.action_space.sample()\n",
        "        observation, reward, done, info = env.step(action)\n",
        "\n",
        "        display.clear_output(wait=True) # 描画のための記述\n",
        "        images.append(env.render(mode=\"rgb_array\")) # 描画のための記述\n",
        "\n",
        "        if done: env.reset()\n",
        "\n",
        "    plt.figure() # 描画のための記述\n",
        "    moment = plt.imshow(images[0]) # 描画のための記述\n",
        "    def update(i):\n",
        "        moment.set_data(images[i])\n",
        "    generatedAnimation = animation.FuncAnimation(plt.gcf(), update, frames=len(images), interval=50) # 描画のための記述\n",
        "    display.display(display.HTML(generatedAnimation.to_jshtml())) # 描画のための記述\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3BPgj-tbU-e"
      },
      "source": [
        "### 環境遷移の画像化"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGlLs25KWE6K"
      },
      "source": [
        "上のプログラムでは動画をグーグルコラボラトリー上で表示しましたが，次のプログラムを実行すると動画を GIF ファイルとして保存することができます．画像の保存方法も 2 個紹介しますが，その 1 個目です．グーグルコラボラトリーの左にあるサイドバー上のフォルダのアイコンをクリックすると保存されたファイルを確認することができます．そのファイル名をダブルクリックすると画面右に動画が表示されます．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cf2_Re6eW7zc"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "import gym\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import animation\n",
        "\n",
        "def main():\n",
        "    env = gym.make('CartPole-v1')\n",
        "\n",
        "    images = []\n",
        "    ax = plt.gca()\n",
        "    ax.axes.xaxis.set_visible(False)\n",
        "    ax.axes.yaxis.set_visible(False)\n",
        "    observation = env.reset()\n",
        "    for _ in range(100):\n",
        "        action = env.action_space.sample()\n",
        "        observation, reward, done, info = env.step(action)\n",
        "\n",
        "        image = plt.imshow(env.render(mode=\"rgb_array\"))\n",
        "        images.append([image])\n",
        "\n",
        "        if done: env.reset()\n",
        "\n",
        "    generatedAnimation = animation.ArtistAnimation(plt.gcf(), images, interval=15, blit=True)\n",
        "    generatedAnimation.save(\"cartpole-01.gif\", writer=\"pillow\", fps=50) # ここだけ変化．\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gah-bJu7XfRh"
      },
      "source": [
        "もう 1 個の画像の生成方法です．以下のようなプログラムを実行します．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0SNcB9cgZo5d"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "import gym\n",
        "import imageio\n",
        "\n",
        "def main():\n",
        "    env = gym.make(\"MountainCar-v0\")\n",
        "\n",
        "    images = []\n",
        "    observation = env.reset()\n",
        "    for _ in range(100):\n",
        "        action = env.action_space.sample()\n",
        "        observation, reward, done, info = env.step(action)\n",
        "\n",
        "        images.append(env.render(mode=\"rgb_array\"))\n",
        "\n",
        "        if done: env.reset()\n",
        "\n",
        "    imageio.mimsave(\"cartpole-02.gif\", images, \"GIF\", **{'duration': 1/50})\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcBYglYFPMvY"
      },
      "source": [
        "### 環境のインポート"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkL0BysHPT0m"
      },
      "source": [
        "```{note}\n",
        "この項の記述はこの教材を実行する際に必要なものではありません．もし他の色々なゲームを導入したい場合にやってみてください．\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7C353LLO3AD"
      },
      "source": [
        "この gym で実際に呼び出せる環境はデフォルトのままだととても少ないです．Atari の ROM を入手してそれを呼び出したい場合は以下のようにします．最初に以下のコマンドで Atari Learning Environment をインストールします．\n",
        "\n",
        "```\n",
        "! pip install ale-py\n",
        "```\n",
        "\n",
        "次に ROM ファイルを入れたフォルダをグーグルコラボラトリーを利用しているユーザーのグーグルドライブに置きます．グーグルドライブのフォルダをグーグルコラボラトリーから参照できるように以下のコードを実行します．\n",
        "\n",
        "```python\n",
        "#!/usr/bin/env python3\n",
        "from google.colab import drive\n",
        "\n",
        "def main():\n",
        "    drive.mount(\"/content/drive\", force_remount=True)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "```\n",
        "\n",
        "そのフォルダの名前が `atari2600` であったとします．その場合，以下のようなコマンドで ROM ファイルをインポートします．\n",
        "\n",
        "```\n",
        "! ale-import-roms drive/MyDrive/atari2600/\n",
        "```\n",
        "\n",
        "この結果，以下のような Atari に由来する環境を呼び出せるようになります．\n",
        "\n",
        "```python\n",
        "#!/usr/bin/env python3\n",
        "import gym\n",
        "\n",
        "def main():\n",
        "    env = gym.make(\"MontezumaRevengeDeterministic-v4\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sD4N6qje9TwB"
      },
      "source": [
        "## 深層 Q 学習"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgHioec6iJS-"
      },
      "source": [
        "Q 学習に深層学習法を利用した深層 Q 学習という方法を紹介します．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCIkDQK74Q2_"
      },
      "source": [
        "### 深層学習と Q 学習"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DaaDthrkRmWW"
      },
      "source": [
        "上の節で紹介した Q 学習法は環境（や行動）のサイズが大きくなるととても大きな計算時間が必要となります．すべての場合における Q テーブルを作成するからです．それを避けるために，Q テーブルを深層学習法を用いて近似しようという試みがありますが，それが深層 Q 学習法です．英語だと Deep Q Network（DQN）と言います．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEKWBxpfRWnB"
      },
      "source": [
        "### 教師データ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rrvf2JDh29mn"
      },
      "source": [
        "深層 Q 学習は Q テーブルをニューラルネットワークによって近似する方法です．よって計算の過程でニューラルネットワークを成長させます．どうやって成長させるかですが，実はとても言い難いことなのですが，実は，そのニューラルネットワークは教師あり学習法によって成長させられます．\n",
        "\n",
        "Q 値の更新式を再掲します．更新された Q 値を $Q'$，環境遷移前の Q 値を $Q$，状態を $s$，行動を $a$，獲得した報酬を $r$，環境遷移後の状態を $s'$，環境遷移後にとり得る行動を $a'$ と書きます．\n",
        "\n",
        "$\n",
        "\\displaystyle Q'(s,a)=Q(s,a)+\\alpha(r+\\gamma\\max Q(s',a')-Q(s,a))\n",
        "$\n",
        "\n",
        "これにたいしてニューラルネットワークの学習に用いる教師データは $r+\\gamma\\max Q(s',a')$ の部分です．停止条件に到達した場合は，$s'$ も $a'$ もないので，$r$ のみが教師データになります．深層 Q 学習に登場するニューラルネットワークはこの教師データと同じような出力ができるように成長します．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gyFzRPCRgWLG"
      },
      "source": [
        "```{note}\n",
        "強化学習なのに教師あり学習．不思議ですね．摩訶不思議です．\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggZjdsJ9ZTPz"
      },
      "source": [
        "### 計算の概要"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OxiBKMRbZVMv"
      },
      "source": [
        "深層 Q 学習がどのように計算されるかについて紹介します．エージェントは 2 個のニューラルネットワークを持ちます．ひとつは Q ネットワークと呼ばれるもので，もうひとつはターゲットネットワークと呼ばれるものです．Q ネットワークは Q 値を学習するためのネットワークです．学習の最中にもニューラルネットワークによって Q 値は常に計算され続けるのですが，その計算にこのネットワークを使うのは良くなさそうです．なぜなら，報酬を教師データとしてその都度成長させられるネットワークを使って，別の状態における Q 値を計算しようとすると，常に別の条件（パラメータ）によって Q 値を計算してしまうからです．よって，パラメータ更新の頻度が少ない別のネットワークを利用して Q 値を計算しますが，それがターゲットネットワークです．\n",
        "\n",
        "1.   環境の初期化をする．\n",
        "2.   エージェントによる行動選択をする．このときのネットワークにはターゲットネットワークを利用する．\n",
        "3.   環境を進めて状態を得る．また，状態を経験バッファ（後で説明）に溜める．\n",
        "4.   経験バッファに溜めたデータを用いて Q ネットワークを学習させる．\n",
        "5.   停止条件に達したら環境を停止．\n",
        "6.   ターゲットネットワークの更新（Q ネットワークのパラメータに同期）．\n",
        "7.   上の 1 から 6 を繰り返す．\n",
        "\n",
        "実際の実装においては，性能を向上させるためのいくつかのテクニックを利用します．次の項でそれらを紹介します．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRNBsE9L2CMl"
      },
      "source": [
        "```{note}\n",
        "学習の過程で Q ネットワークは状態の遷移が起こる度にパラメータの更新がされます．最終的な価値を得るのではなく即時的な報酬を得る度に更新されるということです．そうではなくてエピソードが終了して価値の計算が終わってはじめて，その情報に基づいてエージェントを成長させたいということが開発者のやりたいことです．ターゲットネットワークはその情報に基づいて学習が行われるネットワークです．\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSWhTD-cRbNF"
      },
      "source": [
        "### 性能向上テクニック"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nq1lmVgRprr"
      },
      "source": [
        "前述のように**ターゲットネットワークの利用**をすることで常にパラメータが成長させられる Q ネットワークによって Q 値を生成するという不安定さを関係することができます．ターゲットネットワークの構造は Q ネットワークと完全に一致します．\n",
        "\n",
        "**経験再生法**（**Experience Replay**）は強化学習の最中にミニバッチ学習法を利用すための方法です．環境を進めることで，状態やそのときの行動や停止判定の情報を得ることができますが，その都度にネットワークのパラメータを更新するのではなく，ある一定の情報（経験）が収集された時点ではじめて学習を行う方法です．この経験をどれだけ蓄積するかはハイパーパラメータです．その経験を溜めるものを経験バッファと呼びます．以下のコードでは最大で 512 個の経験を溜めます．新しい経験を記憶した時点で古い記憶を消します．\n",
        "\n",
        "イプシロングリーディ法はイプシロンの確率でエージェントにランダムな行動を選択させる方法ですが，この**イプシロンの減衰**をさせることで学習を経るにつれてランダムな行動を減らすという戦略をとることがあります．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQvJsVydS4kS"
      },
      "source": [
        "### 取り組む問題"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ku3hVhBFS6hh"
      },
      "source": [
        "この節では上の節で紹介した CartPole というゲームを解きます．台車の上に設置されている棒が倒れないように台車を動かすゲームです．エージェントが選択できる行動は「台車を右に動かす」または「台車を左に動かす」の 2 個です．環境を進めることで得られる状態は前述のように「台車の位置」，「台車の速さ」，「棒の角度」，「棒の角速度」の 4 個です．ゲームの停止条件は「棒の角度が 12 度，または，-12 度より大きく，または，小さくなったとき」，「台車の位置がディスプレイの端に位置（2.4 または -2.4）したとき」，「500 単位時間ゲームが続いたとき」です．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgpXiyK7Rq-e"
      },
      "source": [
        "### 深層 Q 学習法の実装"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvIjJlA0RxOF"
      },
      "source": [
        "この問題を解決するために以下のような深層 Q 学習法の実装コードを実行します．少々行儀が悪いのですが，強化学習が終わった直後にテストを行ってその様子を可視化します．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_PCOonwIRxaB"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "from IPython import display\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import animation\n",
        "import tensorflow as tf\n",
        "import random\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "import gym\n",
        "random.seed(0)\n",
        "np.random.seed(0)\n",
        "tf.random.set_seed(0)\n",
        "\n",
        "def main():\n",
        "    # ハイパーパラメータの設定．\n",
        "    epsilon = 0.3 # イプシロングリーディ法のハイパーパラメータ．\n",
        "    epsilonDecayRate = 0.99 # イプシロンの値を学習が進むにつれて小さくするための値．\n",
        "    minimumEpsilon = 0.06 # イプシロンの値はこの値以上は維持する．\n",
        "    gamma = 0.9\n",
        "    replaySize = 512 # 記憶を溜めるサイズ．\n",
        "    minibatchSize = 32\n",
        "    middleUnitSize = 128\n",
        "    dropoutRate = 0.2\n",
        "    \n",
        "    # 環境の生成．\n",
        "    env = gym.make(\"CartPole-v1\")\n",
        "    env.seed(0) # 再現性確保のため乱数の種は絶対に指定する．\n",
        "    \n",
        "    # 環境情報の取得．\n",
        "    actionShape = env.action_space.n # ニューラルネットワークの出力サイズを指定するため取得．\n",
        "    observationShape = env.observation_space.shape # ニューラルネットワークの入力サイズを指定するため取得．\n",
        "    \n",
        "    # リプレイバッファの生成．\n",
        "    experiences = deque(maxlen=replaySize) # 記憶を溜めるためのリスト．\n",
        "    \n",
        "    # エージェントの生成．\n",
        "    agent = Agent(epsilon, epsilonDecayRate, minimumEpsilon, gamma, actionShape, observationShape, middleUnitSize, minibatchSize, dropoutRate)\n",
        "      \n",
        "    for episode in range(1, 200+1):\n",
        "        observation = env.reset()\n",
        "        rewards, costs = [], [] # エピソード毎に報酬とニューラルネットワークの学習コストを溜めるリスト．\n",
        "        while True:\n",
        "            action = agent.act(observation) # エージェントによる行動の選択．これはターゲットネットワークによる選択．\n",
        "            newObservation, reward, done, _ = env.step(action) # 環境を進める．\n",
        "            rewards.append(reward)\n",
        "            experiences.append({\"observation\": observation, \"action\": action, \"reward\": reward, \"newObservation\": newObservation, \"done\": done}) # 経験を蓄積．\n",
        "            observation = newObservation\n",
        "            if len(experiences) >= minibatchSize: # ミニバッチのサイズに達するまで経験がたまったらニューラルネットワークの学習を開始する．\n",
        "                cost = agent.learn(experiences) # Qネットワーク（qModel）の学習．\n",
        "                costs.append(cost)\n",
        "            if done: break\n",
        "        if len(experiences) >= minibatchSize:\n",
        "            agent.update() # エピソードの最後にターゲットネットワーク（targetModel）の更新．\n",
        "        print(\"Episode: {:3d}, Number of steps: {:3d}, Mean reward: {:3.1f}, Cost: {:5.3f}\".format(episode, len(rewards), np.mean(rewards), np.mean(costs)))\n",
        "        \n",
        "    # 以下はテスト結果を可視化するため．\n",
        "    observation = env.reset()\n",
        "    done = False\n",
        "    frame = plt.imshow(env.render(\"rgb_array\"))\n",
        "    frames = [[frame]]\n",
        "    while not done:\n",
        "        action = agent.act(observation)\n",
        "        observation, reward, done, _ = env.step(action)\n",
        "        frame = plt.imshow(env.render(mode=\"rgb_array\")) # 描画のための記述\n",
        "        frames.append([frame])\n",
        "    generatedAnimation = animation.ArtistAnimation(plt.gcf(), frames, interval=15, blit=True) # 描画のための記述\n",
        "    display.display(display.HTML(generatedAnimation.to_jshtml())) # 描画のための記述\n",
        "    generatedAnimation.save(\"cartpole-01.gif\", writer=\"pillow\", fps=50)\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self, epsilon=0.3, epsilonDecayRate=0.99, minimumEpsilon=0.06, gamma=0.9, actionShape=None, observationShape=None, middleUnitSize=16, minibatchSize=32, dropoutRate=0.5):\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilonDecayRate = epsilonDecayRate\n",
        "        self.minimumEpsilon = minimumEpsilon\n",
        "        self.gamma = gamma\n",
        "        self.actionShape = actionShape\n",
        "        self.observationShape = observationShape\n",
        "        self.qModel = Network(middleUnitSize, self.actionShape, dropoutRate) # Q値をその都度学習するためのネットワーク．\n",
        "        self.targetModel = Network(middleUnitSize, self.actionShape, dropoutRate) # Q値を計算するためのネットワーク．エピソードが終わる度にQネットワークと同じパラメータに同期される．\n",
        "        self.minibatchSize = minibatchSize\n",
        "        self.mseComputer = tf.keras.losses.MeanSquaredError() # Q値は右に動かすか左に動かすかの値なのでそれと同じになるように二乗誤差をコストとする．元の論文だとフーバーロスを利用．\n",
        "        self.optimizer = tf.keras.optimizers.Adam()\n",
        "    \n",
        "    # 以下の関数はイプシロングリーディ法を利用して行動を選択する関数．\n",
        "    def act(self, observation):\n",
        "        if np.random.uniform() < self.epsilon:\n",
        "            action = np.random.randint(self.actionShape) # イプシロンの確率でランダムに行動する．\n",
        "        else:\n",
        "            action = np.argmax(self.targetModel(observation.reshape(1,-1), False)[0].numpy()) # 最もQ値が高い行動を選択．予測はターゲットネットワークで行う．\n",
        "        self.epsilonDecay() # イプシロンの値を少しずつ小さくする．\n",
        "        return action\n",
        "    \n",
        "    # 以下の関数はイプシロンを少しずつ小さくするためのもの．\n",
        "    def epsilonDecay(self):\n",
        "        self.epsilon = self.epsilon * self.epsilonDecayRate\n",
        "        if self.epsilon < self.minimumEpsilon:\n",
        "            self.epsilon = self.minimumEpsilon\n",
        "    \n",
        "    @tf.function\n",
        "    def run(self, tx, tt, flag):\n",
        "        with tf.GradientTape() as tape:\n",
        "            self.qModel.trainable = flag # ここで学習させるのはQネットワークの方なのでQネットワークの記述．\n",
        "            ty = self.qModel.call(tx, flag)\n",
        "            costvalue=self.mseComputer(tt, ty) # コストを計算．\n",
        "        gradient = tape.gradient(costvalue, self.qModel.trainable_variables) # 勾配の計算．\n",
        "        self.optimizer.apply_gradients(zip(gradient, self.qModel.trainable_variables)) # 最適化．\n",
        "        return costvalue\n",
        "    \n",
        "    # 以下の関数はQネットワークの学習のため．\n",
        "    def learn(self, experiences):\n",
        "        instances = random.sample(experiences, self.minibatchSize) # リプレイバッファからミニバッチサイズ分のデータを抽出．\n",
        "        observationInstances = np.asarray([instance[\"observation\"] for instance in instances]) # ニューラルネットワークの入力値を取り出しているだけ．\n",
        "        qValues = self.targetModel(observationInstances, False).numpy() # Q値を計算（Q値を格納するための変数を作っただけ）．\n",
        "        newObservationInstances = np.asarray([instance[\"newObservation\"] for instance in instances]) # ニューラルネットワークの入力値を取り出しているだけ．\n",
        "        newQValues = self.targetModel(newObservationInstances, False).numpy() # Q値を計算．\n",
        "        # 以下のforは教師データを作成するための記述．\n",
        "        for i, instance in enumerate(instances):\n",
        "            action = instance[\"action\"]\n",
        "            reward = instance[\"reward\"]\n",
        "            if instance[\"done\"]:\n",
        "                qValues[i][action] = reward\n",
        "            else:\n",
        "                qValues[i][action] = reward + self.gamma * np.max(newQValues[i])\n",
        "        learncostvalue = self.run(observationInstances, qValues, True)\n",
        "        return learncostvalue\n",
        "    \n",
        "    # 以下の関数はターゲットネットワークの更新（Qネットワークと同じにすること）のため．\n",
        "    def update(self):\n",
        "        self.targetModel.set_weights(self.qModel.get_weights())\n",
        "\n",
        "class Network(tf.keras.Model):\n",
        "    def __init__(self, middleUnitSize, outputSize, dropoutRate):\n",
        "        super(Network, self).__init__()\n",
        "        self.w1 = tf.keras.layers.Dense(middleUnitSize)\n",
        "        self.w2 = tf.keras.layers.Dense(middleUnitSize)\n",
        "        self.w3 = tf.keras.layers.Dense(outputSize)\n",
        "        self.a1 = tf.keras.layers.LeakyReLU()\n",
        "        self.dropout = tf.keras.layers.Dropout(dropoutRate)\n",
        "    def call(self, x, learningFlag):\n",
        "        y = self.w1(x)\n",
        "        y = self.a1(y)\n",
        "        y = self.dropout(y, training=learningFlag)\n",
        "        y = self.w2(y)\n",
        "        y = self.a1(y)\n",
        "        y = self.dropout(y, training=learningFlag)\n",
        "        y = self.w3(y)\n",
        "        return y\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVSW_AV9nC-0"
      },
      "source": [
        "実行した結果，エピソードに対して，ゲームを継続することができたステップ数，エピソード毎の平均報酬とニューラルネットワークのコストが出力されました．ステップ数は学習を経るにつれて増えていると思います．また，平均報酬は常に `1.0` となっているはずです．ゲーム終了までの間，1 単位時間ゲームを継続すると報酬が 1 ポイントもらえるというシステムなので，平均報酬は `1.0` となるのです．また，テストを実行した結果すると棒が倒れずに維持される動画を確認できると思います．GPU の利用や乱数の種の状況によっては異なる結果が得られているかもしれません．\n",
        "\n",
        "<img src=\"https://github.com/yamada-kd/binds-training/blob/main/image/cartpole.gif?raw=1\" />\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wp8G-U9toZIi"
      },
      "source": [
        "以下の部分ではハイパーパラメータを設定します．\n",
        "\n",
        "```python\n",
        "    # ハイパーパラメータの設定．\n",
        "    epsilon = 0.3 # イプシロングリーディ法のハイパーパラメータ．\n",
        "    epsilonDecayRate = 0.99 # イプシロンの値を学習が進むにつれて小さくするための値．\n",
        "    minimumEpsilon = 0.06 # イプシロンの値はこの値以上は維持する．\n",
        "    gamma = 0.9\n",
        "    replaySize = 512 # 記憶を溜めるサイズ．\n",
        "    minibatchSize = 32\n",
        "    middleUnitSize = 128\n",
        "    dropoutRate = 0.2\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ai_K_DyXoy6B"
      },
      "source": [
        "以下の部分では環境を静止して，環境の情報を取得して，経験バッファを生成します．再現性確保のために乱数の種は指定すべきです．\n",
        "\n",
        "```python\n",
        "    # 環境の生成．\n",
        "    env = gym.make(\"CartPole-v1\")\n",
        "    env.seed(0) # 再現性確保のため乱数の種は絶対に指定する．\n",
        "    \n",
        "    # 環境情報の取得．\n",
        "    actionShape = env.action_space.n # ニューラルネットワークの出力サイズを指定するため取得．\n",
        "    observationShape = env.observation_space.shape # ニューラルネットワークの入力サイズを指定するため取得．\n",
        "    \n",
        "    # リプレイバッファの生成．\n",
        "    experiences = deque(maxlen=replaySize) # 記憶を溜めるためのリスト．\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gb2A-gVSpMiE"
      },
      "source": [
        "```{note}\n",
        "何をするにしても乱数の種は絶対に指定しましょう．\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_VbA0iJpUoz"
      },
      "source": [
        "以下の部分はエージェントを生成するためのものです．\n",
        "\n",
        "```python\n",
        "    # エージェントの生成．\n",
        "    agent = Agent(epsilon, epsilonDecayRate, minimumEpsilon, gamma, actionShape, observationShape, middleUnitSize, minibatchSize, dropoutRate)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nh9U8yiopYy_"
      },
      "source": [
        "エージェントはクラス `Agent` によって生成されますが，これについて説明します．以下の部分でこのクラスで利用する変数等を生成します．ネットワークは Q ネットワークとターゲットネットワークの 2 個を生成します．\n",
        "\n",
        "```python\n",
        "    def __init__(self, epsilon=0.3, epsilonDecayRate=0.99, minimumEpsilon=0.06, gamma=0.9, actionShape=None, observationShape=None, middleUnitSize=16, minibatchSize=32, dropoutRate=0.5):\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilonDecayRate = epsilonDecayRate\n",
        "        self.minimumEpsilon = minimumEpsilon\n",
        "        self.gamma = gamma\n",
        "        self.actionShape = actionShape\n",
        "        self.observationShape = observationShape\n",
        "        self.qModel = Network(middleUnitSize, self.actionShape, dropoutRate) # Q値をその都度学習するためのネットワーク．\n",
        "        self.targetModel = Network(middleUnitSize, self.actionShape, dropoutRate) # Q値を計算するためのネットワーク．エピソードが終わる度にQネットワークと同じパラメータに同期される．\n",
        "        self.minibatchSize = minibatchSize\n",
        "        self.mseComputer = tf.keras.losses.MeanSquaredError() # Q値は右に動かすか左に動かすかの値なのでそれと同じになるように二乗誤差をコストとする．元の論文だとフーバーロスを利用．\n",
        "        self.optimizer = tf.keras.optimizers.Adam()\n",
        "```\n",
        "\n",
        "以下の部分はイプシロングリーディ法の記述ですが，このコードにおいてはイプシロン減衰も行うのでその記述もあります．\n",
        "\n",
        "```python\n",
        "    # 以下の関数はイプシロングリーディ法を利用して行動を選択する関数．\n",
        "    def act(self, observation):\n",
        "        if np.random.uniform() < self.epsilon:\n",
        "            action = np.random.randint(self.actionShape) # イプシロンの確率でランダムに行動する．\n",
        "        else:\n",
        "            action = np.argmax(self.targetModel(observation.reshape(1,-1), False)[0].numpy()) # 最もQ値が高い行動を選択．予測はターゲットネットワークで行う．\n",
        "        self.epsilonDecay() # イプシロンの値を少しずつ小さくする．\n",
        "        return action\n",
        "    \n",
        "    # 以下の関数はイプシロンを少しずつ小さくするためのもの．\n",
        "    def epsilonDecay(self):\n",
        "        self.epsilon = self.epsilon * self.epsilonDecayRate\n",
        "        if self.epsilon < self.minimumEpsilon:\n",
        "            self.epsilon = self.minimumEpsilon\n",
        "```\n",
        "\n",
        "以下の部分は Subclassing API のいつもの記述です．注意すべき点は，これで学習させられるネットワークは Q ネットワークの方であるということです．\n",
        "\n",
        "```python\n",
        "    @tf.function\n",
        "    def run(self, tx, tt, flag):\n",
        "        with tf.GradientTape() as tape:\n",
        "            self.qModel.trainable = flag # ここで学習させるのはQネットワークの方なのでQネットワークの記述．\n",
        "            ty = self.qModel.call(tx, flag)\n",
        "            costvalue=self.mseComputer(tt, ty) # コストを計算．\n",
        "        gradient = tape.gradient(costvalue, self.qModel.trainable_variables) # 勾配の計算．\n",
        "        self.optimizer.apply_gradients(zip(gradient, self.qModel.trainable_variables)) # 最適化．\n",
        "        return costvalue\n",
        "```\n",
        "\n",
        "以下の記述は Q ネットワークのパラメータ更新のための記述です．経験バッファからミニバッチサイズ分のインスタンスを取り出し，そこから状態と次の状態を抽出し，それに対してターゲットネットワークを利用して Q 値を計算します．その Q 値を教師データとして Q ネットワークを学習させます．\n",
        "\n",
        "```python\n",
        "    # 以下の関数はQネットワークの学習のため．\n",
        "    def learn(self, experiences):\n",
        "        instances = random.sample(experiences, self.minibatchSize) # リプレイバッファからミニバッチサイズ分のデータを抽出．\n",
        "        observationInstances = np.asarray([instance[\"observation\"] for instance in instances]) # ニューラルネットワークの入力値を取り出しているだけ．\n",
        "        qValues = self.targetModel(observationInstances, False).numpy() # Q値を計算（Q値を格納するための変数を作っただけ）．\n",
        "        newObservationInstances = np.asarray([instance[\"newObservation\"] for instance in instances]) # ニューラルネットワークの入力値を取り出しているだけ．\n",
        "        newQValues = self.targetModel(newObservationInstances, False).numpy() # Q値を計算．\n",
        "        # 以下のforは教師データを作成するための記述．\n",
        "        for i, instance in enumerate(instances):\n",
        "            action = instance[\"action\"]\n",
        "            reward = instance[\"reward\"]\n",
        "            if instance[\"done\"]:\n",
        "                qValues[i][action] = reward\n",
        "            else:\n",
        "                qValues[i][action] = reward + self.gamma * np.max(newQValues[i])\n",
        "        learncostvalue = self.run(observationInstances, qValues, True)\n",
        "        return learncostvalue\n",
        "```\n",
        "\n",
        "以下の記述はターゲットネットワークを更新するためのものです．ターゲットネットワークの更新方法は，Q ネットワークのパラメータをそのまま利用するハードアップデートと一部を利用するソフトアップデートがあるのですが，ここではハードアップデートを利用しました．\n",
        "\n",
        "```python\n",
        "    # 以下の関数はターゲットネットワークの更新（Qネットワークと同じにすること）のため．\n",
        "    def update(self):\n",
        "        self.targetModel.set_weights(self.qModel.get_weights())\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kncWV53YrP1I"
      },
      "source": [
        "さらに，`main()` に戻って以下の記述ですが，これが強化学習を進めるためのものです．環境をリセットして，エージェントによる行動の選択，経験バッファに経験を溜めて，Q ネットワークを学習，エピソード終了後にターゲットネットワークを更新します．特に難しいところはないように思います．\n",
        "\n",
        "```python\n",
        "    for episode in range(1, 100+1):\n",
        "        observation = env.reset()\n",
        "        rewards, costs = [], [] # エピソード毎に報酬とニューラルネットワークの学習コストを溜めるリスト．\n",
        "        while True:\n",
        "            action = agent.act(observation) # エージェントによる行動の選択．これはターゲットネットワークによる選択．\n",
        "            newObservation, reward, done, _ = env.step(action) # 環境を進める．\n",
        "            rewards.append(reward)\n",
        "            experiences.append({\"observation\": observation, \"action\": action, \"reward\": reward, \"newObservation\": newObservation, \"done\": done}) # 経験を蓄積．\n",
        "            observation = newObservation\n",
        "            if len(experiences) >= minibatchSize: # ミニバッチのサイズに達するまで経験がたまったらニューラルネットワークの学習を開始する．\n",
        "                cost = agent.learn(experiences) # Qネットワーク（qModel）の学習．\n",
        "                costs.append(cost)\n",
        "            if done: break\n",
        "        if len(experiences) >= minibatchSize:\n",
        "            agent.update() # エピソードの最後にターゲットネットワーク（targetModel）の更新．\n",
        "        print(\"Episode: {:3d}, Number of steps: {:3d}, Mean reward: {:3.1f}, Cost: {:5.3f}\".format(episode, len(rewards), np.mean(rewards), np.mean(costs)))\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkjuyyMjsBUm"
      },
      "source": [
        "最後の以下の部分はテスト結果を可視化するための記述です．本来は学習とテストのコードは分けた方が良いです．\n",
        "\n",
        "```python\n",
        "    # 以下はテスト結果を可視化するため．\n",
        "    observation = env.reset()\n",
        "    done = False\n",
        "    frame = plt.imshow(env.render(\"rgb_array\"))\n",
        "    frames = [[frame]]\n",
        "    while not done:\n",
        "        action = agent.act(observation)\n",
        "        observation, reward, done, _ = env.step(action)\n",
        "        frame = plt.imshow(env.render(mode=\"rgb_array\")) # 描画のための記述\n",
        "        frames.append([frame])\n",
        "    generatedAnimation = animation.ArtistAnimation(plt.gcf(), frames, interval=15, blit=True) # 描画のための記述\n",
        "    display.display(display.HTML(generatedAnimation.to_jshtml())) # 描画のための記述\n",
        "    generatedAnimation.save(\"cartpole-01.gif\", writer=\"pillow\", fps=50)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rfC8Kj03omW"
      },
      "source": [
        "```{note}\n",
        "終わりです．\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}